# -*- coding: utf-8 -*-
"""Project__mood_classfication_using_Kmean.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17fXvRCEPVgoiA_KhZIOUqAv0oG-HkHzb
"""

# Commented out IPython magic to ensure Python compatibility.
# import required libraries

import pandas as pd
import numpy as np
import math
import operator
import plotly.express as px 
import matplotlib.pyplot as plt
import seaborn as sns
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

from yellowbrick.target import FeatureCorrelation
from scipy.stats import pearsonr
from sklearn.cluster import KMeans
from sklearn.preprocessing import Normalizer, MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA


# %matplotlib inline

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

file_id = '1Zc1uGGDidnAkzbbAKCTDa2ZVb71khBf_'

downloaded = drive.CreateFile({'id':file_id}) 
downloaded.GetContentFile('data.csv')  
info = pd.read_csv('data.csv')

info.head()

# check for null emplty value
info.isna().sum().sum()

#  find duplicate value
dup_all = info[info.duplicated(subset = 'track_id', keep=False)]
dup = info[info.duplicated(subset = 'track_id', keep='first')]

info[info['track_id'] == dup['track_id'].iloc[0]]

print(f'''Unique Duplicates count: {dup.shape[0]}
Total Duplicates count: {dup_all.shape[0]}
Total Data count: {info.shape[0]}
Duplicates %: {round(dup_all.shape[0]/info.shape[0]*100, 2)}''')

# remove duplicate values
info.drop_duplicates(subset="track_id", keep=False, inplace=True)

dup_all = info[info.duplicated(subset = 'track_id', keep=False)]
dup = info[info.duplicated(subset = 'track_id', keep='first')]

# info[info['track_id'] == dup['track_id'].iloc[0]]

print(f'''Unique Duplicates count: {dup.shape[0]}
Total Duplicates count: {dup_all.shape[0]}
Total Data count: {info.shape[0]}''')
# Duplicates %: {round(dup_all.shape[0]/info.shape[0]*100, 2)}''')

info.info()

# remove unneccesary columns
unused_columns = ['genre','artist_name', 'track_name', 'track_id']
info = info.drop(columns=unused_columns).reset_index(drop=True)
info.head()

info.select_dtypes(exclude='number').head()

info['key'].unique().tolist()

info['mode'].unique().tolist()

info['time_signature'].unique().tolist()

mode_dictionary = {'Minor' : 0, 'Major' : 1}
key_dictionary = {'A' : 1, 'A#' : 2, 'B' : 3, 'C' : 4, 'C#' : 5, 'D' : 6, 'D#' : 7, 'E' : 8, 'F' : 9, 
                  'F#' : 10, 'G' : 11, 'G#' : 12}

info['time_signature'] = info['time_signature'].apply(lambda x : int(x[0]))
info['mode'].replace(mode_dictionary, inplace=True)
info['key'] = info['key'].replace(key_dictionary).astype(int)

info.head()

info.columns

# print(info.columns())
scaler = StandardScaler()
info =  scaler.fit_transform(info)
info = pd.DataFrame(info,columns=['popularity', 'acousticness', 'danceability', 'duration_ms', 'energy',
       'instrumentalness', 'key', 'liveness', 'loudness', 'mode',
       'speechiness', 'tempo', 'time_signature', 'valence'])

# feature distribution
info.hist(figsize=(8,8))

# P-Values on songs feature
def cal_p_values(info):
    info = info.dropna()._get_numeric_data()
    info_cols = pd.DataFrame(columns=info.columns)
    p_values = info_cols.transpose().join(info_cols, how='outer')
    for row in info.columns:
        for col in info.columns:
            p_values[row][col] = round(pearsonr(info[row], info[col])[1], 4)
    return p_values

cal_p_values(info)

ft_names = ['acousticness',	'danceability',	'duration_ms'	,'energy',	'instrumentalness',	'key',	'liveness',	'loudness',	'mode',	'speechiness',	'tempo',	'time_signature',	'valence']

x_label, y_label = info[ft_names], info['energy']

# Create a list of the feature names
ft = np.array(ft_names)

# Instantiate the visualizer
vis = FeatureCorrelation(labels=ft)

plt.rcParams['figure.figsize']=(10,10)
vis.fit(x_label, y_label)     # Fit the data to the visualizer
vis.show()

# correlation heatmap
# def corr_hm(info):
#     corr_mat = info.corr()
#     mk = np.zeros_like(corr_mat, dtype=np.bool)
#     mk[np.triu_indices_from(mk)]= True
#     fig, axis = plt.subplots(figsize=(11, 15)) 
#     hm = sns.heatmap(corr_mat, 
#                           mask = mk,
#                           square = True,
#                           linewidths = .5,
#                           cmap = 'coolwarm',
#                           cbar_kws = {'shrink': .4, 
#                                     'ticks' : [-1, -.5, 0, 0.5, 1]},
#                           vmin = -1, 
#                           vmax = 1,
#                           annot = True,
#                           annot_kws = {'size': 12})#add the column names as labels
#     axis.set_yticklabels(corr_mat.columns, rotation = 0)
#     axis.set_xticklabels(corr_mat.columns)
#     sns.set_style({'xtick.bottom': True}, {'ytick.left': True})

# corr_hm(info)
# correlation heatmap
plt.figure(figsize=(25,25))
cor_heat = info.corr()
# print(cor_heat)
sns.heatmap(cor_heat, 
            xticklabels=cor_heat.columns.values,
            yticklabels=cor_heat.columns.values,
           annot=True,
           linewidth=0.5)

info.sort_values('key', inplace=True)
new_key = len(info['key'].unique())

# ['acousticness',	'danceability',	'duration_ms'	,'energy',	'instrumentalness',	'key',	'liveness',	'loudness',	'mode',	'speechiness',	'tempo',	'time_signature',	'valence']
# Energy vs. Acousticness
sns.lmplot(data=info, x="acousticness", y="energy", hue="key", palette='tab20',
           fit_reg=False, legend=True, size=8, scatter_kws={'alpha':0.25, 's':12})
plt.xlim(0,1.0)
plt.ylim(0,1.0)
plt.title("Energy vs. Acousticness")

# Instrumentalness vs. Acousticness
sns.lmplot(data=info, x="acousticness", y="instrumentalness", hue="key", palette="tab20",
           fit_reg=False, legend=True, size=8, 
           scatter_kws={'alpha':0.5, 's':25})
plt.xlim(0,1.0)
plt.ylim(0,1.0)
plt.title("Instrumentalness vs. Acousticness")

# Danceability vs. Valence
sns.lmplot(data=info, x="valence", y="danceability", hue="key", palette="tab20",
           fit_reg=False, legend=True, size=8,
          scatter_kws={'alpha':0.25, 's':12})
plt.xlim(0,1.0)
plt.ylim(0,1.0)
plt.title("Danceability vs. Valence")

# Valence vs. Energy
sns.lmplot(data=info, x="energy", y="valence", hue="key", palette="tab20",
           fit_reg=False, legend=True, size=8, 
          scatter_kws={'alpha':0.25, 's':12})
plt.xlim(0,1.0)
plt.ylim(0,1.0)
plt.title("Valence vs. Energy")

count = info.groupby('key').agg({'key':'count'})['key']
major_count = count[count > 11000].index.values
minor_count = count[count <= 11000].index.values
print(count)
print(f"Major count key: {major_count} \nMinor count key: {minor_count}")

key_group_by = info.groupby('key').agg('mean')
cluts = key_group_by.reset_index().rename({'key':'cluster'}, axis=1)
cluts['method'] = 'Mean'

cluts

info[['acousticness',	'danceability',	'energy',	'instrumentalness',	'liveness',	'speechiness',	'tempo',	'valence']].hist(figsize=(8,8))

# remove again unneccesary columns
unused_columns = ['popularity','duration_ms', 'loudness', 'mode', 'time_signature']
info_new = info.drop(columns=unused_columns).reset_index(drop=True)
info_new

ft = info_new[['acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'speechiness', 'tempo', 'valence']]

elr = np.arange(1, 11)
kmean = [KMeans(n_clusters=i, random_state=1986) for i in elr]
scr = [-kmean[i].fit(X).score(X) for i in range(len(kmean))]
elr.dtype = int
plt.figure(figsize=(8,8))
sns.lineplot(x = elr, y = scr, marker='o')
plt.xlim(0,10)
plt.grid(True)
plt.xlabel('Kmean')
plt.ylabel('Error')
plt.title('Elbow Method')

ft = info_new[['acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'speechiness', 'tempo', 'valence']]
song_col = ['acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'speechiness', 'tempo', 'valence']

km = KMeans(n_clusters=len(major_count), n_init=10, random_state=1986)
km.fit(ft)
labs = km.labels_
cluster_centers = km.cluster_centers_
labs_unq = np.unique(labs)
n_clusters_ = len(labs_unq)
km_clusters = pd.DataFrame(cluster_centers, columns=song_col)
km_clusters['cluster'] = ['KM{}'.format(l) for l in labs_unq]
km_clusters['method'] = 'KMeans'
cluts = pd.concat([cluts, km_clusters])
cols = ['cluster'] + song_col + ['method']
cluts = cluts[cols]

cluts

# heatmap cluster vs feature
plt.figure(figsize=(10,10))
sns.set(font_scale=1.5)
sns.heatmap(
    data=cluts.loc[cluts.method=='KMeans', song_col],
    cmap='Purples',
    annot=True
)
plt.ylabel("KMeansLabel")

# KM0: sad
# KM1: eneregetic
# KM2: happy
# KM3: calm
# KM4: party
# KM5: romantic

descriptive_labels = ["sad", "eneregetic", "happy", "calm", "party", "romantic"]
unique_labels = np.unique(labs)
translated_labels = dict(zip(unique_labels, descriptive_labels))
info_new['KMeansLabel'] = list(map(lambda x:translated_labels[x], labs))


genre_count = info_new.groupby('key').agg({'key':'count'})['key']

pca_ft = PCA(n_components=2)
ft_new = pca_ft.fit_transform(X)
x_data,y_data = zip(*ft_new)
info_new['x_data'] = x_data
info_new['y_data'] = y_data
compts = pca_ft.components_
exp_var = pca_ft.explained_variance_
exp_var_ratio = pca_ft.explained_variance_ratio_
sing_vals = pca_ft.singular_values_

song_components = dict(list(zip(song_col, zip(*np.round(compts,2)))))
song_components

compts

sing_vals

# sns.set(font_scale=1.5)
# sns.lmplot(data=info_new, x='x', y='y', hue='key', fit_reg=False, legend=True, size=10, palette='Set1',
#            scatter_kws={'alpha':0.35, 's':25})
# # plt.legend(loc='lower right')
# # plt.ylim(-0.7,1.8)
# # plt.xlim(-0.7,1.8)
# plt.title("Clustering by Genre, Reduced Dimension")
# arrow = plt.arrow(x=0.2, y=-0.40, dx=0.4, dy=0, width=0.04, label='More acoustic, less energy')
# arrow.label = 'More acoustic, less energy'

sns.set(font_scale=1.5)

sns.lmplot(data=info_new, x='x_data', y='y_data', hue='KMeansLabel', fit_reg=False, legend=True, size=8, palette='Set1',
           scatter_kws={'alpha':0.35, 's':25})
plt.title("Clustering by KMeans Label, Reduced Dimension")

